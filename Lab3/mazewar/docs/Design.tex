\section{Design}\label{Design}

\subsection{Component List and Description}\label{DesignComponent}

\paragraph*{Naming Service}

In order for players to communicate with each other without a central server processing all of their request, they must know how many players are there, and how to communicate with them. This is where the Naming Service component comes in. This service maintains a hash map of all currently connected players as well as their hostname and port address. This hash map is updated on player join/leave, and is supplied to newly created player for them to recognize current players. See Section \ref{DesignGameFlow} for details usage of this hash map.

\paragraph*{Client Broadcaster}

For other players to acknowledge movement of local player without a centralized broadcast server, the player must have some way of deliver this information. In our design, we decentralize the role of the broadcast server by splitting it onto each current player. On each local player action such as move, turn, fire or kill, a broadcast packet (See Section \ref{DesignProtocol} for packet structure) will be sent to all current player detailing the movement of the local player. The broadcaster will obtain a hash map containing all current players from the Naming Service, and loop through all the output stream to deliver each packet.

\paragraph*{Client Listener}

To receive packets broadcasted from other players, a separate thread is created to listen on each input stream from each player. When a packet arrives, it temporarily puts on hold until it can be determined that the ordering is correct. After that, the packet is processed, and the action specified in the packet is performed to update the local environment.

\paragraph*{Client Holdback Queue}

Since there is no centralized component that can maintain order of the overall system, client must determine locally what the ordering of arriving packet should be. This however could result in inconsistent game state. As a result, this design implements a queue that holds incoming packet until it can be determined that the next packet played back from the queue is consistent across the game. See Section \ref{DesignConsistency} for details on how the ordering is determined.

\subsection{Communication Protocol}\label{DesignProtocol}

To determine the communication protocol between clients, many options were exploited. The first method we looked at was using DatagramSocket class. This class allows broadcasting packet to all party listening on a particular socket without having to loop through all the output stream. However, this class extends a UDP implementation, and does not guarantee message delivery. One possible solution was to employ at-most-once message delivery algorithm described in class, but without centralized server, the complexity increases dramatically. In addition, the problem of creating a single thread for each input stream is still undealt.

The second method is the one used in our design. Each player creates a socket for every existing player, and allow them to connect and send packet. To deal with blocking on listening on the input stream, a thread is created for every input stream. Clearly there is overhead in this method that the number of threads needed is high, but since modern operating system handle multi-threading quite well, this method should be efficient for our context. 

\paragraph*{Packet Structure} The packets sent between the clients need to contain information about all aspects of the game. This includes player creation, action, acknowledgement, error handling as well as action sequencing. Code \ref{CodeMazewarPacket} shows the proposed packet structure. 
\newpage
{
\singlespacing
\begin{lstlisting}[caption = {[MazewarPacket.java]Structure of packets exchanged between players}, label = CodeMazewarPacket]
public class MazewarPacketIdentifier implements Serializable {
    public int lamportClk;
    public String owner;

    @Override
    public boolean equals(Object id) {
        return (this.lamportClk == ((MazewarPacketIdentifier)id).lamportClk && this.owner.equals(((MazewarPacketIdentifier)id).owner));
    }

    @Override
    public int hashCode() {
        int hash = 1;
        hash = 31 * hash + lamportClk;
        hash = 31 * hash + owner.hashCode();
        return hash;
    }
}
public class MazewarPacket extends MazewarPacketIdentifier implements Serializable, Comparable<MazewarPacket> {
    public static final int NULL = 0;
    public static final int ERROR_DUPLICATED_CLIENT = 1;

    public static final int REGISTER = 100;
    public static final int REGISTER_SUCCESS = 101;

    public static final int ADD_NOTICE = 200;
    public static final int REPORT_LOCATION = 201;
    public static final int ADD = 202;

    public static final int MOVE_FORWARD = 300;
    public static final int MOVE_BACKWARD = 301;

    public static final int TURN_LEFT = 302;
    public static final int TURN_RIGHT = 303;

    public static final int FIRE = 400;
    public static final int INSTANT_KILL = 401;
    public static final int KILL = 402;

    public static final int QUIT = 500;

    //packet definitions
    public String newClient;
    public String ACKer;
    public String victim;
    public int seqNum;

    public InetSocketAddress address;
    public HashMap<String, InetSocketAddress> clientAddresses;

    public int type = MazewarPacket.NULL;
    public DirectedPoint directedPoint;
    public Integer score;

    // Additional variable to indicate number of clients that is multicasted with this packet
    public int cardinality;

    @Override
    public int compareTo(MazewarPacket o) {
        if (this.seqNum == o.seqNum)
            return this.owner.compareTo(o.owner);
        else
            return this.seqNum > o.seqNum ? 1 : -1;
    }
}
}
\end{lstlisting}
}

The packet identifier class identifies a packet uniquely by its lamport clock and the client's name who generated the packet, and in the case where two clients are involved (e.g. killing), killer will generate the packet and victim's name is recorded in the victim field. Packet contents contains a type variable that identifies the action the packet aims to perform, such as moving or turning. ACKer is the name of the client that acknowledges an action packet. This field is null for action packets. Seqnum is the lamport clock for the packet when the sender sends it. In case of an action packe, it is the same as the lamportClk field. But in the case of an ack packet, it is usually different from the lamportClk field since lamportClk field is to identify the original action packet. Address is the ip address of the sender client to register at the naming service. Client addresses is a map returned from naming service that includes all the connected clients' names and ip addresses. Directied point variable is used to supply clients with a new location of a particular player in case of respawning or new player joining. Score variable is used to update new joined player with current score table. Cardinality variable is to indicate the number of connected clients at the moment of multicasting. LamportClk and seqnum field together are used to maintain a total ordering of packet delivery acorss all players, the detail of this design is presented in Section \ref{DesignConsistency}.

\subsection{How a player Locate, Leave and Join}\label{DesignGameFlow}

This section puts all the components together to describe the life cycle of the game play. First of all, when a player wishes to join the game, a query is sent to the naming service which replies with a list of currently connected player, and the socket address they are listening on. The new player then establishes connection with all provided sockets, and wait for the other players to acknowledge this join request. When the existing player receives a new player request, it registers the new connection locally, and replies with its own current location. After the new player have received reply from everyone else it then starts to process any action related packet in the hold-back queue based on sequence number.

When a player decides to leave, it sends a broadcast packet with type QUIT as in Code \ref{CodeMazewarPacket}, and waits for acknowledgement from all players. When all the players acknowledge the quit, the player can then gracefully exit. This is to maintain total order as described in Section \ref{DesignConsistency}.

\subsection{Maintaining Consistency}\label{DesignConsistency}

Consistency is a major issue in a distributed game in this case. To demonstrate possible scenario of inconsistent state, we will look at several cases. First of all, consider player A and player B both made a move to position (12,7). Player B will see its own movement first and the broadcast packet from player A second, and declare that player B has arrived at (12,7) first. Similarly player A will think itself arrived first, resulting in two different game state. Second of all, we look at two players that are right next to each other firing at each other at the same time. This is quite a frequent scenario since there are many corners that lead to players facing each other. Similar to the first case, the local players might see different order of event resulting in differnt player being vaporized on different machine. Thirdly, when a new player joins dynamically, it must capture the current game state, and put all subsequent movement on hold before the current game state being processed and displayed. This again requires an ordering of events across the game.

To solve this issue, we realized that an ordering needs to be established. Several types of ordering in a distributed system were looked at. First, the FIFO ordering only guarantees that if process A produces a happens-before relation, then this relation is captured on all other processes broadcasted to, it provides no information on order acorss processes. Secondly, the Causal ordering were looked at, which guarantees if message m -$>$ m' where -$>$ is the happened-before relation induced, then any other process will also deliver m before m'. This again does not guarantee two processes will have same order of execution. Finally we arrive at total ordering, which guarantees that the order of execution will be maintained throughout the system.

Total ordering leads to a consistent game state at all times across all players, and should be applied as part of this design. We should note that total ordering does not guarantee fairness in real time. For example, if player A and B are next to each other and fires at one another, if player A fired first, and player B second, total ordering does not guarantee that player A will always kill player B. Instead, it guarantees that if player B killed player A, everyone in the game will see the same state. To achieve total fairness, a physical clock synchronization is needed.
